# SpecGen: Legacy SPSS to Modern ETL Compiler


> **A semantic compiler that reverse-engineers legacy SPSS syntax into a platform-agnostic Intermediate Representation (IR).**

`SpecGen` is not a transpiler; it is a **logic extractor**. It parses legacy scripts, builds a dependency graph of data transformations, and exports a clean YAML specification that can be used to generate SQL, R, or PySpark pipelines.

---

## ğŸ— Architecture

The project follows a strict **Compiler Frontend** architecture, designed to separate syntax parsing from semantic understanding.

```mermaid
graph LR
    A[Raw SPSS File] -->|Lexer| B(Tokens)
    B -->|Parser| C(Abstract Syntax Tree)
    C -->|Graph Builder| D{Intermediate Representation}
    D -->|Exporter| E[YAML Artifact]

```

### 1. The Ingestion Layer

* **Lexer (`src/importers/spss/lexer.py`):** A robust tokenizer that handles SPSS-specific quirks (e.g., trailing dots, case insensitivity) using strict Regex definitions.
* **Parser (`src/importers/spss/parser.py`):** A recursive descent parser that converts tokens into a type-safe **Abstract Syntax Tree (AST)**.
* *Feature:* **"Fugue State" Processing** â€” Automatically detects and ignores inline raw data blocks (`BEGIN DATA` ... `END DATA`) to prevent parser hallucinations.
* *Feature:* **Semantic Capture** â€” Parses complex `AGGREGATE` and `MATCH FILES` commands into structured objects, not generic text.



### 2. The Semantic Layer

* **Graph Builder (`src/importers/spss/graph_builder.py`):** converts the linear AST into a Directed Acyclic Graph (DAG) of operations.
* **Data Lineage:** Tracks dataset state across Joins, Filters, and Aggregations.
* **Deterministic IDs:** Generates stable IDs for datasets and operations, ensuring consistent output across runs.



### 3. The Validation Layer (`src/ir`)

* **Pydantic Models:** Enforces strict typing on the IR.
* **Self-Healing:** Automatically detects cycles in the graph and validates that all inputs/outputs reference existing datasets.

---

## ğŸš€ Getting Started

### Prerequisites

* Python 3.12+
* `pydantic`, `networkx`, `pyyaml`

### Installation

```bash
git clone https://github.com/your-repo/spec_generator.git
cd spec_generator
pip install -r requirements.txt

```

### Usage

Run the compiler against any SPSS syntax file:

```bash
PYTHONPATH=src:. python cli.py path/to/script.sps

```

This will generate a `pipeline_spec.yaml` in the output directory.

---

## ğŸ§ª Testing Strategy

This project maintains **96% Code Coverage** and employs a multi-tiered testing strategy:

1. **Unit Tests (`tests/unit`):** Isolate specific components (e.g., verifying `AGGREGATE` parsing handles lists correctly).
2. **Integration Tests (`tests/integration`):** Validate the full pipeline lifecycle.
* `test_advanced_scenarios.py`: The "Kitchen Sink" test that runs Load -> Compute -> Filter -> Aggregate -> Join -> Save.


3. **Technical Debt Tracking (`test_pending_features.py`):** A suite of tests designed to *document* missing features. These tests assert that commands like `RECODE` are currently "Generic," serving as a reminder for future development.

**Run the full suite:**

```bash
PYTHONPATH=src:. pytest --cov=src --cov-report=term-missing

```

---

## ğŸ“‚ Project Structure

```text
src/
â”œâ”€â”€ importers/spss/      # The Compiler Frontend
â”‚   â”œâ”€â”€ grammar.py       # Regex definitions for Tokens
â”‚   â”œâ”€â”€ lexer.py         # Tokenizer logic
â”‚   â”œâ”€â”€ ast.py           # AST Node definitions (ComputeNode, AggregateNode, etc.)
â”‚   â”œâ”€â”€ parser.py        # Recursive Descent Parser
â”‚   â””â”€â”€ graph_builder.py # Logic for transforming AST -> IR Graph
â”œâ”€â”€ ir/                  # Intermediate Representation
â”‚   â”œâ”€â”€ model.py         # Pydantic models (Pipeline, Operation, Dataset)
â”‚   â””â”€â”€ types.py         # Enums (OpType, DataType)
â””â”€â”€ exporters/           # Backend Code Generators
    â””â”€â”€ yaml.py          # Serializes the IR to YAML

```

---

## ğŸ”® Roadmap & Technical Debt

While the compiler is robust for core ETL tasks, advanced statistical transformations are currently treated as "Black Boxes."

| Feature | Status | Notes |
| --- | --- | --- |
| **Logic & Math** | âœ… Complete | Full support for `COMPUTE`, `IF`, `SELECT IF`. |
| **Aggregation** | âœ… Complete | White-box support for `BREAK` variables and formulas. |
| **Joins** | âœ… Complete | Support for `MATCH FILES` / `STAR JOIN`. |
| **Raw Data** | âœ… Complete | Safely ignores `BEGIN DATA` blocks. |
| *RECODE* | âš ï¸ Generic | Currently parsed as a generic passthrough. |
| *DATA LIST* | âš ï¸ Generic | Implicit schema definition not yet fully captured. |

---

## ğŸ“ Example Output

Input SPSS:

```spss
AGGREGATE /OUTFILE=* /BREAK=region /total = SUM(sales).

```

Output YAML (IR):

```yaml
- id: op_020_aggregate
  type: aggregate
  inputs: [ds_019]
  outputs: [ds_020]
  parameters:
    outfile: '*'
    break: ['region']
    aggregations: ['TOTAL = SUM(SALES)']

```
